{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfenriswulven\u001b[0m (\u001b[33mdtu_projects\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\ollie\\OneDrive - Danmarks Tekniske Universitet\\Uni\\Bachelor Projekt\\Normalising-Flow-DNN\\postnet\\wandb\\run-20231129_013342-d2kf0tkv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dtu_projects/Normalising-Flow-DNN/runs/d2kf0tkv' target=\"_blank\">bright-wood-98</a></strong> to <a href='https://wandb.ai/dtu_projects/Normalising-Flow-DNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dtu_projects/Normalising-Flow-DNN' target=\"_blank\">https://wandb.ai/dtu_projects/Normalising-Flow-DNN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dtu_projects/Normalising-Flow-DNN/runs/d2kf0tkv' target=\"_blank\">https://wandb.ai/dtu_projects/Normalising-Flow-DNN/runs/d2kf0tkv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'postnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ollie\\OneDrive - Danmarks Tekniske Universitet\\Uni\\Bachelor Projekt\\Normalising-Flow-DNN\\postnet\\1coupling_moons.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ollie/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Uni/Bachelor%20Projekt/Normalising-Flow-DNN/postnet/1coupling_moons.ipynb#W0sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m wandb\u001b[39m.\u001b[39minit(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ollie/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Uni/Bachelor%20Projekt/Normalising-Flow-DNN/postnet/1coupling_moons.ipynb#W0sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     project\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNormalising-Flow-DNN\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ollie/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Uni/Bachelor%20Projekt/Normalising-Flow-DNN/postnet/1coupling_moons.ipynb#W0sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     config\u001b[39m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ollie/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Uni/Bachelor%20Projekt/Normalising-Flow-DNN/postnet/1coupling_moons.ipynb#W0sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m     \u001b[39m#job_type='train'\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ollie/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Uni/Bachelor%20Projekt/Normalising-Flow-DNN/postnet/1coupling_moons.ipynb#W0sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m )\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ollie/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Uni/Bachelor%20Projekt/Normalising-Flow-DNN/postnet/1coupling_moons.ipynb#W0sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m \u001b[39m############ CLASSES ################\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/ollie/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Uni/Bachelor%20Projekt/Normalising-Flow-DNN/postnet/1coupling_moons.ipynb#W0sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpostnet\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mPosteriorNetwork_class\u001b[39;00m \u001b[39mimport\u001b[39;00m PosteriorNetwork, Conditioner, AffineCouplingLayer, NormalisingFlow\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ollie/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Uni/Bachelor%20Projekt/Normalising-Flow-DNN/postnet/1coupling_moons.ipynb#W0sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpostnet\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mEncoder_Moons\u001b[39;00m \u001b[39mimport\u001b[39;00m Encoder_Moons\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ollie/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Uni/Bachelor%20Projekt/Normalising-Flow-DNN/postnet/1coupling_moons.ipynb#W0sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpostnet\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mEncoder_MNIST\u001b[39;00m \u001b[39mimport\u001b[39;00m Encoder_MNIST\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'postnet'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.nn.utils as utils\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.distributions import Distribution\n",
    "from torch.distributions.dirichlet import Dirichlet\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import io\n",
    "import math\n",
    "from typing import Tuple, List, Callable\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import utils as vutils\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import gaussian_kde\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "import wandb\n",
    "import os \n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'Coupling_mnist_plots.ipynb'\n",
    "# set seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "############ HYPERPARAMETERS ################\n",
    "\n",
    "#lr = 0.00005\n",
    "training_lr = 5e-5\n",
    "start_lr = 1e-9\n",
    "min_lr = 1e-7 # during cosine annealing\n",
    "num_epochs = 200 # flere epochs maybe 12000\n",
    "warmup_steps= 500\n",
    "validation_every_steps = 50 # is actually every epoch in training loop!!\n",
    "#validation_every_epochs = 1\n",
    "weight_decay = 1e-7  # L2 regularization strength to prevent overfitting in Adam or AdamW \n",
    "batch_size = 64\n",
    "early_stop_delta = 0.0001\n",
    "early_stop_patience = 12\n",
    "split = lambda x: x.chunk(2, dim=-1)\n",
    "reg = 5e-5#  entropy regularisation\n",
    "annealing_interval = 200 # Every 10 epochs, anneal LR (warm restart)\n",
    "\n",
    "num_classes = 2\n",
    "latent_dim = 2 # the encoder outputs 2D latent space\n",
    "data_dim = 2 # the encoder outputs 2D latent space\n",
    "in_dim= data_dim // 2 # since we split the data\n",
    "out_dim= data_dim // 2\n",
    "num_params = 2 # s and t\n",
    "num_hidden = 2 # number of hidden layers\n",
    "hidden_dim = 32 # neurons in hidden layers\n",
    "num_flows = 4 # number of coupling flow layers\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    project='Normalising-Flow-DNN',\n",
    "    config={\n",
    "        'architecture': 'PostNet',\n",
    "        'dataset': 'TwoMoons',\n",
    "        'training_lr': training_lr,\n",
    "        'start_lr': start_lr,\n",
    "        'min_lr': min_lr,\n",
    "        'num_epochs': num_epochs,\n",
    "        'warmup_steps': warmup_steps,\n",
    "        'validation_every_steps': validation_every_steps,\n",
    "        'weight_decay': weight_decay,\n",
    "        'batch_size': batch_size,\n",
    "        'early_stop_delta': early_stop_delta,\n",
    "        'early_stop_patience': early_stop_patience,\n",
    "        'reg': reg,\n",
    "        'annealing_interval': annealing_interval,\n",
    "        'num_classes': num_classes,\n",
    "        'latent_dim': latent_dim,\n",
    "        'num_params': num_params,\n",
    "        'num_hidden': num_hidden,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'num_flows': num_flows,\n",
    "    }\n",
    "    #name='run_name',\n",
    "    #tags=['experiment1', ''],\n",
    "    #notes='Trying out a new architecture',\n",
    "    #dir='/path/to/log/files',\n",
    "    ##entity='my_team',\n",
    "    #group='experiment_group',\n",
    "    #job_type='train'\n",
    ")\n",
    "\n",
    "############ CLASSES ################\n",
    "\n",
    "from postnet.PosteriorNetwork_class import PosteriorNetwork, Conditioner, AffineCouplingLayer, NormalisingFlow\n",
    "from postnet.Encoder_Moons import Encoder_Moons\n",
    "from postnet.Encoder_MNIST import Encoder_MNIST\n",
    "from postnet.Encoder_CIFAR import Encoder_CIFAR\n",
    "\n",
    "    \n",
    "############ LOAD MOONS DATASET ################\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Generate the \"make_moons\" dataset\n",
    "X, y = make_moons(n_samples=3000, noise=0.1, random_state=42)\n",
    "\n",
    "# 20% test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "# Create PyTorch datasets and data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "# Dictionary with class names\n",
    "classes = {0: \"Class 0\", 1: \"Class 1\"}\n",
    "\n",
    "# Initialize dictionary to store the counts for each class\n",
    "N = {class_idx: (y_train == class_idx).sum().item() for class_idx in classes}\n",
    "N = torch.tensor([N[class_idx] for class_idx in classes])\n",
    "\n",
    "############# FUNCTION ##################\n",
    "\n",
    "def init_weights(model):\n",
    "    if isinstance(model, nn.Linear):\n",
    "        nn.init.xavier_normal_(model.weight)\n",
    "        nn.init.zeros_(model.bias) \n",
    "\n",
    "def accuracy(y_train, preds):\n",
    "    accuracy = accuracy_score(y_train.cpu().numpy(), preds.cpu().numpy())\n",
    "    return accuracy\n",
    "\n",
    "############# INSTANTIATE MODEL ##################\n",
    "flow_models = []\n",
    "for class_label in range(num_classes):\n",
    "    conditioner = Conditioner(in_dim=in_dim, out_dim=out_dim, num_hidden=num_hidden, hidden_dim=hidden_dim, num_params=num_params)\n",
    "    affine_coupling = AffineCouplingLayer(conditioner, split=split) # split the tensor into 2 parts\n",
    "    flows = [affine_coupling for _ in range(num_flows)]\n",
    "    latent_distribution = torch.distributions.MultivariateNormal(loc=torch.zeros(data_dim).to(device), scale_tril=torch.eye(data_dim).to(device)) #maybe move out of loop?\n",
    "\n",
    "    flow_model = NormalisingFlow(latent_distribution, flows).to(device)\n",
    "    #flow_model = NormalisingFlow(latent_distribution, flows).apply(init_weights).to(device)\n",
    "    flow_models.append(flow_model)\n",
    "\n",
    "postnet_model = PosteriorNetwork(latent_dim, flow_models, N, num_classes, y_train, reg).to(device) \n",
    "optimiser = optim.AdamW(postnet_model.parameters(), lr=training_lr, weight_decay=weight_decay)\n",
    "\n",
    "class GradualWarmupScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_steps, start_lr, end_lr, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.start_lr = start_lr\n",
    "        self.end_lr = end_lr\n",
    "        self.lr_increment = (end_lr - start_lr) / warmup_steps\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.warmup_steps:\n",
    "            lr = self.start_lr + self.last_epoch * self.lr_increment\n",
    "            return [lr for _ in self.base_lrs]\n",
    "        return self.base_lrs\n",
    "warmup_scheduler = GradualWarmupScheduler(optimiser, warmup_steps=warmup_steps, start_lr=start_lr, end_lr=training_lr)\n",
    "\n",
    "total_steps_per_epoch = len(train_loader)  # Total batches (steps) per epoch\n",
    "warmup_epochs = math.ceil(warmup_steps / total_steps_per_epoch)  # Total warmup epochs\n",
    "#T_max = num_epochs - warmup_epochs  # Total number of training epochs minus warmup epochs\n",
    "print(\"total_steps_per_epoch\", total_steps_per_epoch)\n",
    "print(\"Warmup epochs:\", warmup_epochs)\n",
    "#print(\"Training epochs T_max: \", T_max)\n",
    "# rounded up to make sure all warmup steps are used before AnnealingLR\n",
    "\n",
    "#T_max = 10\n",
    "training_scheduler = lr_scheduler.CosineAnnealingLR(optimiser, T_max=annealing_interval, eta_min=min_lr, last_epoch=-1)\n",
    "#training_scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=5, gamma=0.1)\n",
    "#training_scheduler = ExponentialLR(optimizer=default_optimiser, gamma=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_z(model, X_train, y_train):\n",
    "    latent_train_data = model.cnn(X_train.to(device)).cpu().detach().numpy()\n",
    "    y_train = y_train.cpu().detach().numpy()\n",
    "    # Create a buffer to save the plot\n",
    "    buf = io.BytesIO()\n",
    "    \n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(latent_train_data[y_train== 0][:, 0], latent_train_data[y_train== 0][:, 1], c='red', label='True Class 0', marker='o')\n",
    "    plt.scatter(latent_train_data[y_train== 1][:, 0], latent_train_data[y_train== 1][:, 1], c='green', label='True Class 1', marker='o')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title('Latent Space')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    image = plt.imread(buf, format='png')\n",
    "    wandb.log({\"Latent representations\": wandb.Image(image)})\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## TRAINING ######################\n",
    "def train(model, optimiser, train_loader, test_loader, num_epochs, validation_every_steps, \n",
    "          early_stop_delta, early_stop_patience, warmup_scheduler, training_scheduler, warmup_steps):\n",
    "    model.train()\n",
    "    train_losses, train_accuracies, val_losses, val_accuracies = [], [], [], []\n",
    "    all_train_losses = []\n",
    "    best_val_loss = float(\"Inf\")\n",
    "    step = 0 # how many batches we have trained on (each batch is 64 samples) #9000 training samples / 64 batch size = 140 batches per epoch\n",
    "    counter = 0 # for early stopping \n",
    "    early_stopping = False\n",
    "    wandb.watch(model, log=\"all\")\n",
    "\n",
    "    for epoch in range(num_epochs): #epoch is one forward pass through the entire training set\n",
    "        train_losses_batches, train_accuracies_batches = [], []\n",
    "        #batches_counter = 0\n",
    "\n",
    "        for batch_index, (X_train, y_train) in enumerate(train_loader):\n",
    "            X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "            # batches_counter += 1\n",
    "\n",
    "            # Forward pass\n",
    "            alpha = model(X_train, N)\n",
    "            loss = model.loss_postnet(alpha, y_train) #batch size\n",
    "            # Perform one training step\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            utils.clip_grad_norm_(flow_model.parameters(), max_norm=5.0)\n",
    "            \n",
    "            optimiser.step()\n",
    "            if step < warmup_steps:\n",
    "                warmup_scheduler.step()\n",
    "            \n",
    "            step += 1\n",
    "            #train_losses.append(loss.item())\n",
    "\n",
    "            # Compute training accuracy and loss for this batch\n",
    "            with torch.no_grad():\n",
    "                preds = torch.max(alpha, dim=-1)[1]\n",
    "                train_accuracy_batch = accuracy(y_train, preds)\n",
    "                train_accuracies_batches.append(train_accuracy_batch)\n",
    "                train_losses_batches.append(loss.item())\n",
    "                all_train_losses.append(loss.item())\n",
    "                current_lr = optimiser.param_groups[0]['lr']\n",
    "                wandb.log({\"batch_train_losses\": loss.item(), \"batch_train_accuracy\": \n",
    "                           train_accuracy_batch, \"step\": step, \"learning_rate\": current_lr, \"epoch\": epoch})\n",
    "                #wandb.log({\"batch_train_losses\": loss.item(), \"batch_train_accuracy\": \n",
    "                           #train_accuracy_batch, \"step\": step})\n",
    "                \n",
    "                #train_accuracies.append(batch_accuracy)\n",
    "\n",
    "            # if epoch >= warmup_epochs:\n",
    "            #     if (epoch - warmup_epochs) % annealing_interval == 0:\n",
    "            #         training_scheduler.step()\n",
    "\n",
    "            if step % validation_every_steps == 0:\n",
    "                train_loss = np.mean(train_losses_batches)\n",
    "                train_losses.append(train_loss)\n",
    "                train_accuracy = np.mean(train_accuracies_batches)\n",
    "                train_accuracies.append(train_accuracy)\n",
    "                wandb.log({\"train_loss\": train_loss, \"train_accuracy\": train_accuracy, \"step\": step})\n",
    "\n",
    "                val_losses_batches = []\n",
    "                #val_accuracies_batches = []\n",
    "                val_correct = []\n",
    "                model.eval()\n",
    "                with torch.no_grad():   \n",
    "                    for batch_index, (X_test, y_test) in enumerate(test_loader):\n",
    "                        X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "                        # Evaluation Forward pass\n",
    "                        alpha = model(X_test, N) # gives a vector with alphas for each class\n",
    "                        loss = model.loss_postnet(alpha, y_test) #gives a loss\n",
    "                        \n",
    "                        # Evaluation accuracy and loss for this batch\n",
    "                        preds = torch.max(alpha, dim=-1)[1]\n",
    "                        \n",
    "                        correct_batch = (preds == y_test).sum().item() \n",
    "                        val_correct.append(correct_batch)\n",
    "\n",
    "                        #Maybe: Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=False).\n",
    "                        #val_accuracies_batches.append(accuracy(y_test, preds) * len(X_test))\n",
    "\n",
    "                        # append the loss for this batch\n",
    "                        val_losses_batches.append(loss.item())\n",
    "\n",
    "                val_accuracy = sum(val_correct) / len(y_test) #Multiply by len(test_dataset) because the final batch of DataLoader may be smaller (drop_last=False).\n",
    "                val_accuracies.append(val_accuracy)\n",
    "                val_loss = np.mean(val_losses_batches) \n",
    "                val_losses.append(val_loss)\n",
    "                wandb.log({\"val_loss\": val_loss, \"val_accuracy\": val_accuracy, \"step\": step})\n",
    "                model.train()\n",
    "\n",
    "                if val_losses[-1] < -1.:\n",
    "                    print(\"Unstable training\")\n",
    "                    break\n",
    "                if np.isnan(val_losses[-1]):\n",
    "                    print('Detected NaN Loss')\n",
    "                    break\n",
    "                # If val_loss is the best so far, save the model state_dict and reset the early stopping counter\n",
    "                if val_losses[-1] < best_val_loss:\n",
    "                    best_val_loss = val_losses[-1]\n",
    "                    counter = 0\n",
    "                    best_model = model.state_dict()\n",
    "                    torch.save({'epoch': epoch, 'model_state_dict': best_model, 'loss': best_val_loss}, 'best_model_moons_entropy.pth')\n",
    "                    print('Model saved')\n",
    "                    # Plot latent space representations z \n",
    "                    plot_latent_z(model, X_train, y_train)\n",
    "                    \n",
    "\n",
    "                # Early stopping - if val_loss is not improving (plus a delta e-4 as buffer) then start counter\n",
    "                # after patience of a certain number of validations, then stop training\n",
    "                elif val_losses[-1] > (best_val_loss + early_stop_delta):\n",
    "                    counter += 1\n",
    "                    if counter >= early_stop_patience:\n",
    "                        #print(\"Early stopping\")\n",
    "                        early_stopping = True\n",
    "                        break\n",
    "                \n",
    "                print(f\"Step: {step}, Epoch: {epoch+1}\\tTrain Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Train Accuracy: {train_accuracies[-1]:.4f}, Val Accuracy: {val_accuracies[-1]:.4f}\")\n",
    "                #### Lave plots med meshgrid f-funktion af normalising flow undervejs for at se ændringen\n",
    "                \n",
    "        # Update training scheduler (annealing LR)\n",
    "        if epoch >= warmup_epochs:\n",
    "                #if (epoch - warmup_epochs) % annealing_interval == 0:\n",
    "                training_scheduler.step()\n",
    "        #current_lr = optimiser.param_groups[0]['lr']\n",
    "        #print(f\"Epoch {epoch}: Current Epoch LR = {current_lr}\")\n",
    "\n",
    "        if early_stopping: # if true\n",
    "            print(\"Early stopping triggered. Exiting training.\")\n",
    "            break  # Break out of the outer loop\n",
    "    print(\"Finished training.\")\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies, all_train_losses #,model\n",
    "\n",
    "train_losses, val_losses, train_accuracies, val_accuracies, all_train_losses = train(postnet_model, optimiser, train_loader, test_loader, \n",
    "                                                num_epochs, validation_every_steps, early_stop_delta, early_stop_patience, warmup_scheduler, \n",
    "                                                training_scheduler, warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log figurer op undervejs - på serveren\n",
    "\n",
    "# efter en fornemmelse af parametrene - tune manuelt først\n",
    "#- har en gridserach og adaptive tuner\n",
    "\n",
    "# learning rate\n",
    "# weight decay - prøve at sætte en hård grænse hvor man ganger med 0.1\n",
    "\n",
    "# ved 15000 steps: dividere learning rate med 10\n",
    "\n",
    "#- Plot two moons og bagrund logprobs for Klassen. Grid er datapunkter – køre gennem encoder – og så gennem flow og regne logprobs og plot der. Kan se hvad modellen gør.\n",
    "\n",
    "#- Plot med outlier og bestemte punkter highlighted. Evt bruge regnbue farver. Husk punkt 5 imellem de to klasser. Out of distribution – aleatorisk usikkerhed. \n",
    "  #  Brug AU-ROC curve til at se thresholds for ood data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss of training and validation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].plot(train_losses, label='Train Loss')\n",
    "axes[0].plot(val_losses, label='Validation Loss')\n",
    "axes[0].set_xlabel('Validation Epochs')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Train and Validation Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot accuracies of training and validation\n",
    "axes[1].plot(train_accuracies, label='Train Accuracy')\n",
    "axes[1].plot(val_accuracies, label='Validation Accuracy')\n",
    "axes[1].set_xlabel('Validation Epochs')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Train and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# all train losses\n",
    "print(len(all_train_losses))\n",
    "# plot all_train_losses\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(all_train_losses,  '.',label='Train Loss', alpha=0.3)\n",
    "#plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model from pth file type\n",
    "device = torch.device(\"cpu\")\n",
    "flow_models = []\n",
    "for class_label in range(num_classes):\n",
    "    conditioner = Conditioner(in_dim=in_dim, out_dim=out_dim, num_hidden=num_hidden, hidden_dim=hidden_dim, num_params=num_params)\n",
    "    affine_coupling = AffineCouplingLayer(conditioner, split=lambda x: x.chunk(2, dim=-1)) # split the tensor into 2 parts\n",
    "    flows = [affine_coupling for _ in range(num_flows)]\n",
    "    latent_distribution = torch.distributions.MultivariateNormal(loc=torch.zeros(data_dim).to(device), scale_tril=torch.eye(data_dim).to(device))\n",
    "\n",
    "    flow_model = NormalisingFlow(latent_distribution, flows, ). to(device)\n",
    "    #flow_model = NormalisingFlow(latent_distribution, flows).apply(init_weights).to(device)\n",
    "    flow_models.append(flow_model)\n",
    "best_model = PosteriorNetwork(latent_dim, flow_models, N, num_classes, y_train, reg).to(device)\n",
    "best_model.load_state_dict(torch.load('best_model_moons_entropy.pth', map_location=device)['model_state_dict'])\n",
    "#best_model.eval()\n",
    "\n",
    "# Test best model on test data\n",
    "N = N.to(device) #move num classes to device just in case\n",
    "alpha_values = []\n",
    "# print(next(best_model.parameters()).is_cuda)\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_index, (X_test_batch, y_test_batch) in enumerate(test_loader):\n",
    "        X_test_batch, y_test_batch = X_test_batch.to(device), y_test_batch.to(device)\n",
    "        # Forward pass to compute alpha values for this batch\n",
    "        alpha_preds = best_model(X_test_batch, N) # batch x num_classes    \n",
    "        # Append the alpha values to the list\n",
    "        #print(alpha_preds.shape)\n",
    "        alpha_values.append(alpha_preds)\n",
    "\n",
    "# Combine alpha values from all batches list into a single tensor\n",
    "alpha_values = torch.cat(alpha_values, dim=0)\n",
    "print(\"Alpha values shape: \", alpha_values.shape)\n",
    "\n",
    "# Normalize alpha values\n",
    "alphas_norm = F.normalize(alpha_values, p=1, dim=1) #reduce to dim 1, sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the latent space\n",
    "latent_train_data = best_model.cnn(X_train_tensor.to(device)).cpu().detach().numpy()\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(latent_train_data[y_train == 0][:, 0], latent_train_data[y_train == 0][:, 1], c='red', label='True Class 0', marker='o')\n",
    "plt.scatter(latent_train_data[y_train == 1][:, 0], latent_train_data[y_train == 1][:, 1], c='green', label='True Class 1', marker='o')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Latent Space')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#Plot med outlier og bestemte punkter highlighted. Evt bruge regnbue farver. Husk punkt 5 imellem de to klasser. Out of distribution – aleatorisk usikkerhed. \n",
    "#  Brug AU-ROC curve til at se thresholds for ood data.\n",
    "# use X_train and colour them rainbow based on x1 value\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "# Plot training data\n",
    "rainbow_colors_c0 = plt.get_cmap('rainbow')(X_train[y_train == 0][:, 0] / X_train[y_train == 0][:, 0].max())\n",
    "rainbow_colors_c1 = plt.get_cmap('rainbow')(X_train[y_train == 1][:, 0] / X_train[y_train == 1][:, 0].max())\n",
    "\n",
    "# axes[0].scatter(X_train[y_train == 0][:, 0], X_train[y_train == 0][:, 1], c='red', label='Class 0')\n",
    "# axes[0].scatter(X_train[y_train == 1][:, 0], X_train[y_train == 1][:, 1], c='green', label='Class 1')\n",
    "axes[0].scatter(X_train[y_train == 0][:, 0], X_train[y_train == 0][:, 1], c=rainbow_colors_c0, label='Class 0')\n",
    "axes[0].scatter(X_train[y_train == 1][:, 0], X_train[y_train == 1][:, 1], c=rainbow_colors_c1, label='Class 1')\n",
    "axes[0].set_xlabel('Input dimension 1')\n",
    "axes[0].set_ylabel('Input dimension 2')\n",
    "axes[0].legend(loc='lower left', frameon=True, labelcolor='black')  # Adjust legend properties\n",
    "axes[0].set_title('Training Data')\n",
    "\n",
    "# plot the latent space datapoints and see how they changed\n",
    "latent_train_data = best_model.cnn(X_train_tensor.to(device)).cpu().detach().numpy()\n",
    "axes[1].scatter(latent_train_data[y_train == 0][:, 0], latent_train_data[y_train == 0][:, 1], c=rainbow_colors_c0, label='True Class 0', marker='o')\n",
    "axes[1].scatter(latent_train_data[y_train == 1][:, 0], latent_train_data[y_train == 1][:, 1], c=rainbow_colors_c1, label='True Class 1', marker='o')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "axes[1].legend(loc='upper left')\n",
    "axes[1].set_title('Latent Space')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the meshgrid of the learned latent space\n",
    "# Create a meshgrid in the latent space\n",
    "coords = 3.8\n",
    "denseness = 700\n",
    "nx, ny = (denseness, denseness)\n",
    "x = np.linspace(-coords, coords, nx)\n",
    "y = np.linspace(-coords, coords, ny)\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "mesh = np.stack((xv.flatten(), yv.flatten()), axis=1)\n",
    "mesh_tensor = torch.FloatTensor(mesh).to(device)\n",
    "\n",
    "# Run the train data through only the CNN using the best_model.pt\n",
    "latent_train_data = best_model.cnn(X_train_tensor.to(device)).cpu().detach().numpy()\n",
    "# Run a meshgrid of datapoints through encoder (and then through flow) to get logprobs\n",
    "latent_mesh = best_model.cnn(mesh_tensor.to(device))\n",
    "\n",
    "num_samples = 1000\n",
    "samples, posterior_samples, generated_samples = [], [], []\n",
    "# Compute log-probabilities for each class\n",
    "logprobs, logprobs_mesh = [], []\n",
    "for i in range(num_classes):\n",
    "    log_prob = flow_models[i].log_prob(mesh_tensor).clamp(min=-10, max=10)\n",
    "    logprobs.append(log_prob.cpu().reshape((nx, ny)).detach().numpy())\n",
    "    # run latent_mesh through flow to get logprobs\n",
    "    log_prob_mesh = flow_models[i].log_prob(latent_mesh).clamp(min=-10, max=10)\n",
    "    logprobs_mesh.append(log_prob_mesh.cpu().reshape((nx, ny)).detach().numpy())\n",
    "\n",
    "    # Get samples in the latent space\n",
    "    samples.append(best_model.flow_models[i].sample(num_samples).to(device)) #self.g(self.latent_sample(num_samples))\n",
    "    # Sample from the posterior distribution using the NormalisingFlow object\n",
    "    post = best_model.flow_models[i].latent_sample(num_samples).to(device)\n",
    "    posterior_samples.append(post)    \n",
    "\n",
    "\n",
    "# Show what the normalising flow has learned\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.imshow(np.exp(logprobs_mesh[0]), extent=[-2, 2, -2, 2], origin='lower', cmap='viridis', alpha=1)\n",
    "plt.imshow(np.exp(logprobs_mesh[1]), extent=[-2, 2, -2, 2], origin='lower', cmap='viridis', alpha=0.5)\n",
    "plt.colorbar(label='Probability Density')\n",
    "plt.xlabel('Input Dimension 1')\n",
    "plt.ylabel('Input Dimension 2')\n",
    "plt.title('Before Encoder Combined Probability Density for Both Classes')\n",
    "plt.show()\n",
    "\n",
    "#Grid er datapunkter – køre gennem encoder – og så gennem flow og regne logprobs og plot der\n",
    "# BEFORE encoder plot the logprobs_mesh as heatmaps for each class\n",
    "colors = ['red', 'green']\n",
    "plt.figure(figsize=(12,4))\n",
    "for class_label in range(num_classes):\n",
    "    plt.subplot(1, num_classes, class_label + 1)\n",
    "    plt.imshow(np.exp(logprobs_mesh[class_label]), extent=[-coords, coords, -coords, coords], origin='lower', cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.scatter(X_train[y_train == class_label][:, 0], X_train[y_train == class_label][:, 1],\n",
    "                c=colors[class_label], alpha=0.07, label=f'True Class {class_label}')\n",
    "    plt.xlabel('Input Dimension 1')\n",
    "    plt.ylabel('Input Dimension 2')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(f'Before encoder Class {class_label} Probability Density map with Train Data ') \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# AFTER encoder Plot the log probabilities as heatmaps for each class\n",
    "colors = ['red', 'red']\n",
    "plt.figure(figsize=(12,4))\n",
    "for class_label in range(num_classes):\n",
    "    plt.subplot(1, num_classes, class_label + 1)\n",
    "    plt.imshow(np.exp(logprobs[class_label]), extent=[-coords, coords, -coords, coords], origin='lower', cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.scatter(latent_train_data[y_train == class_label][:, 0], latent_train_data[y_train == class_label][:, 1],\n",
    "                c=colors[class_label], alpha=0.07, label=f'True Class {class_label}')\n",
    "    plt.xlabel('Latent Dimension 1')\n",
    "    plt.ylabel('Latent Dimension 2')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(f'After encoder Class {class_label} Probability Density with Train Data')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "for class_label in range(num_classes):\n",
    "    plt.subplot(1, num_classes, class_label + 1)\n",
    "    plt.imshow(np.exp(logprobs[class_label]), extent=[-coords, coords, -coords, coords], origin='lower', cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.scatter(samples[class_label][:, 0].numpy(), samples[class_label][:, 1].numpy(), \n",
    "                c=colors[class_label],s=10, alpha=0.07, label=f'Generated Class {class_label}')\n",
    "    plt.xlabel('Latent Dimension 1')\n",
    "    plt.ylabel('Latent Dimension 2')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(f'Class {class_label} Probability Density with Generated \\nSamples from the normalising flow')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot the log probabilities for both classes in the same plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(np.exp(logprobs[0]), extent=[-2, 2, -2, 2], origin='lower', cmap='viridis', alpha=1)\n",
    "plt.imshow(np.exp(logprobs[1]), extent=[-2, 2, -2, 2], origin='lower', cmap='viridis', alpha=0.5)\n",
    "plt.colorbar(label='Probability Density')\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.title('Combined Probability Density for Both Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the meshgrid of the learned latent space\n",
    "# Create a meshgrid in the latent space\n",
    "nx, ny = (500, 500)\n",
    "coords = 2\n",
    "x = np.linspace(-coords, coords, nx)\n",
    "y = np.linspace(-coords, coords, ny)\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "mesh = np.stack((xv.flatten(), yv.flatten()), axis=1)\n",
    "mesh_tensor = torch.FloatTensor(mesh).to(device)\n",
    "\n",
    "# Run the train data through only the CNN using the best_model.pt\n",
    "latent_train_data = best_model.cnn(X_train_tensor.to(device)).cpu().detach().numpy()\n",
    "num_samples = 1000\n",
    "logprobs, generated_samples, posterior_samples = [], [], []\n",
    "# Compute log-probabilities for each class\n",
    "logprobs = []\n",
    "for i in range(num_classes):\n",
    "    log_prob = flow_models[i].log_prob(mesh_tensor).clamp(min=-10, max=10)\n",
    "    logprobs.append(log_prob.cpu().reshape((nx, ny)).detach().numpy())\n",
    "\n",
    "    # Sample from the posterior distribution using the NormalisingFlow object\n",
    "    posterior_samples.append(best_model.flow_models[i].latent_sample(num_samples).to(device))\n",
    "\n",
    "    # Run function g on a latent sample to map it to data space\n",
    "    generated_samples.append(best_model.flow_models[i].sample(num_samples).to(device))\n",
    "\n",
    "# Plot the log probabilities as heatmaps for each class\n",
    "colors = ['red', 'green']\n",
    "plt.figure(figsize=(10, 4))\n",
    "for class_label in range(num_classes):\n",
    "    plt.subplot(1, num_classes, class_label + 1)\n",
    "    plt.imshow(np.exp(logprobs[class_label]), extent=[-coords, coords, -coords, coords], origin='lower', cmap='viridis')\n",
    "    plt.scatter(latent_train_data[y_train == class_label][:, 0], latent_train_data[y_train == class_label][:, 1],\n",
    "                c=colors[class_label], label='True Class {}'.format(class_label), marker='.', alpha=0.02)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Latent Dimension 1')\n",
    "    plt.ylabel('Latent Dimension 2')\n",
    "    plt.title(f'Class {class_label} Probability Density')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create subplots for each class\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "for i, ax_row in enumerate(axes):\n",
    "    ax_row[0].scatter(generated_samples[i][:, 0].numpy(), generated_samples[i][:, 1].numpy(), s=10, alpha=0.5)\n",
    "    ax_row[0].set_xlabel('Feature 1')\n",
    "    ax_row[0].set_ylabel('Feature 2')\n",
    "    ax_row[0].set_title(f'Class {i} Generated Samples')\n",
    "\n",
    "    ax_row[1].scatter(posterior_samples[i][:, 0].numpy(), posterior_samples[i][:, 1].numpy(), s=10, alpha=0.5)\n",
    "    ax_row[1].set_xlabel('Feature 0')\n",
    "    ax_row[1].set_ylabel('Feature 0')\n",
    "    ax_row[1].set_title(f'Class {i} Posterior Samples')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms of the alphas and normalized alphas\n",
    "print(alphas_norm.shape)\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 6))\n",
    "ax[0,0].hist(alpha_values[y_test == 0][:,0], bins=50, label='Alpha 0', alpha=1)\n",
    "#ax[0,0].hist(alpha_values[y_test == 0][:,1], bins=50, label='Alpha 1', alpha=0.5, color='red')\n",
    "ax[0,0].set_title('Histogram of Alphas values for Class 0')\n",
    "ax[0,0].set_xlabel('Alpha')\n",
    "ax[0,0].set_ylabel('Frequency')\n",
    "ax[1,0].hist(alpha_values[y_test == 1][:,1], bins=50)\n",
    "ax[1,0].set_title('Histogram of Alphas values for Class 1')\n",
    "ax[1,0].set_xlabel('Alpha')\n",
    "ax[1,0].set_ylabel('Frequency')\n",
    "ax[0,1].hist(alphas_norm[y_test == 0][:,0], bins=50)\n",
    "ax[0,1].set_title('Histogram of Normalized Alphas values for Class 0')\n",
    "ax[0,1].set_xlabel('Alpha')\n",
    "ax[0,1].set_ylabel('Frequency')\n",
    "ax[1,1].hist(alphas_norm[y_test == 1][:,1], bins=50)\n",
    "ax[1,1].set_title('Histogram of Normalized Alphas values for Class 1')\n",
    "ax[1,1].set_xlabel('Alpha')\n",
    "ax[1,1].set_ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
