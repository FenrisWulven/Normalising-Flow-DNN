{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.nn.utils as utils\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# set seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Callable\n",
    "from torch.distributions import Distribution\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import gaussian_kde\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://sebastiancallh.github.io/post/affine-normalizing-flows/\n",
    "# uses Real NVP paper\n",
    "\n",
    "# Define the Normalising Flow model template\n",
    "class NormalisingFlow(nn.Module):\n",
    "\n",
    "    def __init__(self, latent: Distribution, flows: List[nn.Module]):\n",
    "        super(NormalisingFlow, self).__init__()\n",
    "        self.latent = latent\n",
    "        self.flows = nn.ModuleList(flows)\n",
    "\n",
    "    def latent_log_prob(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        log_prob_z = self.latent.log_prob(z)\n",
    "        return log_prob_z\n",
    "\n",
    "    def latent_sample(self, num_samples: int = 1) -> torch.Tensor:\n",
    "        return self.latent.sample((num_samples,))\n",
    "\n",
    "    def sample(self, num_samples: int = 1) -> torch.Tensor:\n",
    "        '''Sample a new observation x by sampling z from\n",
    "        the latent distribution and pass through g.'''\n",
    "        return self.g(self.latent_sample(num_samples))\n",
    "\n",
    "    def f(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        '''Maps observation x to latent variable z.\n",
    "        Additionally, computes the log determinant\n",
    "        of the Jacobian for this transformation.\n",
    "        Inverse of g.'''\n",
    "        #z, sum_log_abs_det = x, torch.zeros(x.size(0)).to(device) # change to x device\n",
    "        \n",
    "        z, sum_log_abs_det = x, torch.zeros(x.size(0), device = x.device)\n",
    "        for flow in self.flows: # for each transformation in the flow\n",
    "            z, log_abs_det = flow.f(z)\n",
    "            sum_log_abs_det += log_abs_det\n",
    "\n",
    "        return z, sum_log_abs_det\n",
    "\n",
    "    def g(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        '''Maps latent variable z to observation x.\n",
    "        Inverse of f.'''\n",
    "        with torch.no_grad():\n",
    "            x = z\n",
    "            for flow in reversed(self.flows):\n",
    "                x = flow.g(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def g_steps(self, z: torch.Tensor) -> List[torch.Tensor]:\n",
    "        '''Maps latent variable z to observation x\n",
    "        and stores intermediate results.'''\n",
    "        xs = [z]\n",
    "        for flow in reversed(self.flows):\n",
    "            xs.append(flow.g(xs[-1]))\n",
    "\n",
    "        return xs\n",
    "\n",
    "    def log_prob(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        '''Computes log p(x) using the change of variable formula.'''\n",
    "        z, log_abs_det = self.f(x)\n",
    "        log_prob_x = self.latent_log_prob(z) + log_abs_det\n",
    "        return log_prob_x\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.flows)\n",
    "\n",
    "\n",
    "class AffineCouplingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, theta: nn.Module, split: Callable[[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]):\n",
    "        super(AffineCouplingLayer, self).__init__()\n",
    "        self.theta = theta\n",
    "        self.split = split\n",
    "\n",
    "    def f(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        '''f: x -> z. The inverse of g.'''\n",
    "        x2, x1 = self.split(x) # flip the split #insert other permutation for 3D+ data\n",
    "        ### permutation at one point\n",
    "        t, s = self.theta(x1)\n",
    "        # Tau coupling function: e^s + t\n",
    "        z1, z2 = x1, x2 * torch.exp(s) + t\n",
    "        log_det = s.sum(-1) # sum over the last dimension\n",
    "        return torch.cat((z1, z2), dim=-1), log_det\n",
    "\n",
    "    def g(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        '''g: z -> x. The inverse of f.'''\n",
    "        z1, z2 = self.split(z)\n",
    "        t, s = self.theta(z1)\n",
    "        x1, x2 = z1, (z2 - t) * torch.exp(-s)\n",
    "        return torch.cat((x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "class Conditioner(nn.Module):\n",
    "    'The conditioner is the Neural Network that helps fit the model to the data by learning theta_i = (s_i,t_i)'\n",
    "\n",
    "    def __init__(self, in_dim: int, out_dim: int, num_hidden: int, hidden_dim: int, num_params: int):\n",
    "        super(Conditioner, self).__init__()\n",
    "        self.input = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),  \n",
    "            nn.LeakyReLU(inplace=True)  \n",
    "        )\n",
    "        self.hidden = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),  \n",
    "                nn.Dropout(0.3),\n",
    "                nn.LeakyReLU(inplace=True)  \n",
    "            )\n",
    "            for _ in range(num_hidden)\n",
    "        ])\n",
    "\n",
    "        self.num_params = num_params\n",
    "        self.out_dim = out_dim\n",
    "        self.output = nn.Linear(hidden_dim, out_dim * num_params)\n",
    "        # initialisere output lag conditioner alle vægte og bias til 0\n",
    "        nn.init.zeros_(self.output.weight)\n",
    "        nn.init.zeros_(self.output.bias) \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.input(x)\n",
    "        for h in self.hidden:\n",
    "            x = h(x)\n",
    "\n",
    "        batch_params = self.output(x).reshape(x.size(0), self.out_dim, -1)\n",
    "        #batch_params[:,:,1] *= 0.001\n",
    "        #batch_params[:,:,0] *= 0.001 \n",
    "        params = batch_params.chunk(self.num_params, dim=-1)\n",
    "        return [p.squeeze(-1) for p in params]\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "        )\n",
    "\n",
    "        self.linear_block = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, latent_dim),\n",
    "            nn.BatchNorm1d(latent_dim)\n",
    "            # nn.Dropout(p=0.5),\n",
    "            # nn.Linear(128*7*7, 128),\n",
    "            # nn.BatchNorm1d(128),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.Dropout(0.5),\n",
    "            # nn.Linear(128, 64),\n",
    "            # nn.BatchNorm1d(64),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.Dropout(0.5),\n",
    "            # nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "        # self.flat = nn.Flatten()\n",
    "        # # I want the feature space to be 2D?? eller 10 for num_classe\n",
    "        # self.fc1 = nn.Linear(32*8*8, latent_dim) #num_classes\n",
    "        # self.batchnorm1 = nn.BatchNorm1d(latent_dim) #num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        #print(\"Shape after conv_block:\", x.shape) #64batchsize x 64channels x 1height x 1width\n",
    "        #x = x.view(x.size(0), -1) # flatten\n",
    "        x = self.linear_block(x)\n",
    "        return x\n",
    "    \n",
    "  ## Afprøve:\n",
    "# learning rate scheduler\n",
    "# rapport\n",
    "# Langt tid senere: korrekte usikkerheder senere\n",
    "# plot i datarummet - meshgrid - normaliseret alpha eller flows    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosteriorNetwork(nn.Module):\n",
    "    def __init__(self, latent_dim: int, flow_models: List[nn.Module], N: torch.tensor, num_classes: int, y: torch.Tensor):\n",
    "        super(PosteriorNetwork, self).__init__()\n",
    "        self.cnn = CNN(latent_dim)\n",
    "        self.flow_models = nn.ModuleList(flow_models)\n",
    "        self.N = N\n",
    "        self.y = y\n",
    "        self.num_classes = num_classes \n",
    "    \n",
    "    def forward(self, x, N):\n",
    "        batch_size = x.size(0)\n",
    "        # N is number of inputs in each class total\n",
    "        z = self.cnn(x) \n",
    "        # for each class, since outputdim = num_classes\n",
    "        alpha = torch.zeros((batch_size, self.num_classes)).to(z.device.type)\n",
    "        log_q = torch.zeros((batch_size, self.num_classes)).to(z.device.type)\n",
    "        beta_i = torch.zeros((batch_size, self.num_classes)).to(z.device.type)\n",
    "\n",
    "        # for each class, compute \n",
    "        for c in range(self.num_classes):\n",
    "            log_prob = self.flow_models[c].log_prob(z) #P(z|c,phi)\n",
    "            log_q[:,c] = log_prob\n",
    "            beta_prior = 1 \n",
    "            beta_i[:,c] = N[c] * torch.exp(log_prob)  #formula (4) from paper\n",
    "            alpha[:,c] = beta_prior + beta_i[:,c] #or just beta_i[c]?\n",
    "\n",
    "        # grad_loss\n",
    "        #loss = self.loss_postnet(alpha, y, batch_size)\n",
    "\n",
    "        ##alpha = F.normalize(alpha, p=1, dim=1) # to get p^bar_c which is the average of alphas\n",
    "        #preds = self.predict(alphas) #categorical prediction using argmax on p_bar_c\n",
    "        return alpha\n",
    "\n",
    "    def loss_postnet(self, alpha, y, batch_size): #UCE loss \n",
    "        #alpha is the p vector with alphas for each class\n",
    "        #y_hot is the ground-truth class labels\n",
    "        alpha_0 = torch.sum(alpha, dim=1, keepdim=True) #batch x 1\n",
    "        digamma_alpha_c = torch.digamma(alpha[range(batch_size),y]) # batch x 1\n",
    "\n",
    "        digamma_alpha_0 = torch.digamma(alpha_0) # batch x 1, hver obs får sin egent logprobs\n",
    "        uce_loss = digamma_alpha_c - digamma_alpha_0 #elementwise\n",
    "        #uncertain_loss = torch.sum((digamma_alpha_c - digamma_alpha_sum) * alpha, dim=1)\n",
    "        \n",
    "        # entropy in Dirichlet distribution\n",
    "        uce_loss = -torch.mean(uce_loss) #negative since we want to minimize the loss\n",
    "        #approximates the true posterior distribution for the categorical distribution p\n",
    "        return uce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 6000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ollie\\OneDrive - Danmarks Tekniske Universitet\\Uni\\Bachelor Projekt\\Normalising-Flow-DNN\\CouplingFlow_mnist_hpc.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ollie/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Uni/Bachelor%20Projekt/Normalising-Flow-DNN/CouplingFlow_mnist_hpc.ipynb#X54sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m num_samples \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(train_dataset) \u001b[39m*\u001b[39m subset_percentage)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ollie/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Uni/Bachelor%20Projekt/Normalising-Flow-DNN/CouplingFlow_mnist_hpc.ipynb#X54sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNumber of samples:\u001b[39m\u001b[39m\"\u001b[39m, num_samples)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ollie/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Uni/Bachelor%20Projekt/Normalising-Flow-DNN/CouplingFlow_mnist_hpc.ipynb#X54sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m train_subset \u001b[39m=\u001b[39m Subset(train_dataset, \u001b[39mrange\u001b[39;49m(num_samples\u001b[39m*\u001b[39;49m\u001b[39m0.8\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ollie/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Uni/Bachelor%20Projekt/Normalising-Flow-DNN/CouplingFlow_mnist_hpc.ipynb#X54sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m test_subset \u001b[39m=\u001b[39m Subset(test_dataset, \u001b[39mrange\u001b[39m(num_samples\u001b[39m*\u001b[39m\u001b[39m0.2\u001b[39m)) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ollie/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Uni/Bachelor%20Projekt/Normalising-Flow-DNN/CouplingFlow_mnist_hpc.ipynb#X54sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(train_subset, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "batch_size = 64 # standard value\n",
    "from torch.utils.data.dataset import Subset\n",
    "subset_percentage = 0.1\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "num_samples = int(len(train_dataset) * subset_percentage)\n",
    "print(\"Number of samples:\", num_samples)\n",
    "train_subset = Subset(train_dataset, range(num_samples*0.8))\n",
    "test_subset = Subset(test_dataset, range(num_samples*0.2)) \n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Get ground-truth label counts N_c\n",
    "# Dictionary with class names from indexes.\n",
    "classes = {index: name for name, index in train_dataset.class_to_idx.items()}\n",
    "print(classes)\n",
    "\n",
    "# Initialise dictionary to store the counts for each class using class indexes\n",
    "N = {index: 0 for index in range(len(classes))}\n",
    "# Count the occurrences of each class\n",
    "for _, target in train_dataset:\n",
    "    N[target] += 1\n",
    "N = torch.tensor([N[index] for index in range(len(classes))])\n",
    "print(N)\n",
    "\n",
    "y_train = torch.tensor([target for _, target in train_dataset])\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_show(img):\n",
    "    img = img.detach().cpu()\n",
    "    img = img / 2 + 0.5   #Unnormalise\n",
    "    with sns.axes_style(\"white\"):\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(img.permute((1, 2, 0)).numpy())\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "def accuracy(target, pred):\n",
    "    return metrics.accuracy_score(target.detach().cpu().numpy(), pred.detach().cpu().numpy())\n",
    "\n",
    "def init_weights(model):\n",
    "    if isinstance(model, nn.Linear):\n",
    "        nn.init.xavier_normal_(model.weight)\n",
    "        nn.init.zeros_(model.bias) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get random training images and show them.\n",
    "images, labels = next(iter(train_loader))\n",
    "image_show(utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "latent_dim = 2 # the encoder outputs 2D latent space\n",
    "data_dim = 2 # the encoder outputs 2D latent space\n",
    "in_dim= data_dim // 2 # since we split the data\n",
    "out_dim= data_dim // 2\n",
    "num_params = 2 # s and t\n",
    "num_hidden = 2 # number of hidden layers\n",
    "hidden_dim = 32 # neurons in hidden layers\n",
    "num_flows = 4 # number of coupling flow layers\n",
    "\n",
    "#lr = 0.0031\n",
    "lr = 0.01\n",
    "weight_decay = 1e-6  # L2 regularization strength to prevent overfitting in Adam or AdamW \n",
    "num_epochs = 10 # flere epochs maybe 12000\n",
    "batch_size = 64\n",
    "validation_every_steps = 1 # is actually every epoch in training loop!!\n",
    "#validation_every_epochs = 5\n",
    "early_stop_delta = 0.0001\n",
    "early_stop_patience = 15\n",
    "split = lambda x: x.chunk(2, dim=-1) #splits in half\n",
    "\n",
    "# initalise the flows and postnet model\n",
    "flow_models = []\n",
    "for class_label in range(num_classes):\n",
    "    conditioner = Conditioner(in_dim=in_dim, out_dim=out_dim, num_hidden=num_hidden, hidden_dim=hidden_dim, num_params=num_params)\n",
    "    affine_coupling = AffineCouplingLayer(conditioner, split=split) # split the tensor into 2 parts\n",
    "    flows = [affine_coupling for _ in range(num_flows)]\n",
    "    latent_distribution = torch.distributions.MultivariateNormal(loc=torch.zeros(data_dim).to(device), scale_tril=torch.eye(data_dim).to(device)) #maybe move out of loop?\n",
    "\n",
    "    flow_model = NormalisingFlow(latent_distribution, flows).to(device)\n",
    "    #flow_model = NormalisingFlow(latent_distribution, flows).apply(init_weights).to(device)\n",
    "    flow_models.append(flow_model)\n",
    "\n",
    "postnet_model = PosteriorNetwork(latent_dim, flow_models, N, num_classes, y_train).to(device) \n",
    "optimiser = optim.AdamW(postnet_model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## TRAINING ######################\n",
    "##############################################\n",
    "def accuracy(y_train, preds):\n",
    "    accuracy = accuracy_score(y_train.cpu().numpy(), preds.cpu().numpy())\n",
    "    return accuracy\n",
    "\n",
    "####### TO DO : #######\n",
    "## Implement learning rate scheduling - adaptive learning rate during training: step_decay, learning rate annealing or learning rate scheduler\n",
    "## - dividere med 10 på et tidspunkt\n",
    "\n",
    "## Use a hyperparameter tuning library like Optuna or GridSearchCV to automate the search process.\n",
    "## Cross-validation - implement a k-fold cross-validation to evaluate the model performance on the training data.\n",
    "## Check normalisation of data\n",
    "## Check regularization strength - e.g. weight decay or dropout\n",
    "## Ensemble Methods: If applicable, consider using ensemble methods like bagging or boosting by training multiple models and combining their predictions.\n",
    "\n",
    "\n",
    "def train(model, optimiser, train_loader, test_loader, num_epochs, validation_every_steps, early_stop_delta, early_stop_patience):\n",
    "    model.train()\n",
    "    train_losses, train_accuracies, val_losses, val_accuracies = [], [], [], []\n",
    "    all_train_losses = []\n",
    "    best_val_loss = float(\"Inf\")\n",
    "    step = 0 # how many batches we have trained on (each batch is 32 samples) #1600 training samples / 32 batch size = 50 batches per epoch\n",
    "    counter = 0\n",
    "    for epoch in range(num_epochs): #one forward pass through the entire training set\n",
    "\n",
    "        train_losses_batches, train_accuracies_batches = [], []\n",
    "\n",
    "        # batches_counter = 0\n",
    "        for batch_index, (X_train, y_train) in enumerate(train_loader):\n",
    "            X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "            \n",
    "            # batches_counter += 1\n",
    "            # print(f\"Batch: {batches_counter}\")\n",
    "            # print(\"batch size: \", X_train.size(0))\n",
    "            # Forward pass\n",
    "            alpha = model(X_train, N)\n",
    "            loss = model.loss_postnet(alpha, y_train, X_train.size(0)) #batch size\n",
    "            # Perform one training step\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            #utils.clip_grad_norm_(flow_model.parameters(), max_norm=5.0)\n",
    "            optimiser.step()\n",
    "            step += 1\n",
    "            #train_losses.append(loss.item())\n",
    "\n",
    "            # Compute training accuracy and loss for this batch\n",
    "            with torch.no_grad():\n",
    "                preds = torch.max(alpha, dim=-1)[1]\n",
    "                train_accuracies_batches.append(accuracy(y_train, preds))\n",
    "                train_losses_batches.append(loss.item())\n",
    "                #batch_accuracy = accuracy_score(y_train.cpu().numpy(), preds.cpu().numpy())\n",
    "                #train_accuracies.append(batch_accuracy)\n",
    "                all_train_losses.append(loss.item())\n",
    "\n",
    "        if epoch % validation_every_steps == 0:\n",
    "            train_losses.append(np.mean(train_losses_batches))\n",
    "            train_accuracies.append(np.mean(train_accuracies_batches))\n",
    "\n",
    "            val_losses_batches = []\n",
    "            #val_accuracies_batches = []\n",
    "            val_correct = []\n",
    "            model.eval()\n",
    "            with torch.no_grad():   \n",
    "                for batch_index, (X_test, y_test) in enumerate(test_loader):\n",
    "                    X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "                    # Evaluation Forward pass\n",
    "                    alpha = model(X_test, N) # gives a vector with alphas for each class\n",
    "                    loss = model.loss_postnet(alpha, y_test, X_test.size(0)) #gives a loss\n",
    "                    \n",
    "                    # Evaluation accuracy and loss for this batch\n",
    "                    preds = torch.max(alpha, dim=-1)[1]\n",
    "                    \n",
    "                    correct_batch = (preds == y_test).sum().item()\n",
    "                    val_correct.append(correct_batch)\n",
    "\n",
    "                    #Maybe: Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=False).\n",
    "                    #val_accuracies_batches.append(accuracy(y_test, preds) * len(X_test))\n",
    "\n",
    "                    # append the loss for this batch\n",
    "                    val_losses_batches.append(loss.item())\n",
    "\n",
    "            val_accuracy = sum(val_correct) / len(test_dataset) \n",
    "            val_accuracies.append(val_accuracy)\n",
    "\n",
    "            val_loss = np.mean(val_losses_batches) \n",
    "            val_losses.append(val_loss)\n",
    "            #val_losses.append(np.mean(val_losses_batches))\n",
    "            #val_accuracies.append(np.sum(val_accuracies_batches) / len(test_dataset))   \n",
    "\n",
    "            if val_losses[-1] < -1.:\n",
    "                print(\"Unstable training\")\n",
    "                break\n",
    "            if np.isnan(val_losses[-1]):\n",
    "                print('Detected NaN Loss')\n",
    "                break\n",
    "            # If val_loss is the best so far, save the model state_dict and reset the early stopping counter\n",
    "            if val_losses[-1] < best_val_loss:\n",
    "                best_val_loss = val_losses[-1]\n",
    "                counter = 0\n",
    "                best_model = model.state_dict()\n",
    "                torch.save({'epoch': epoch, 'model_state_dict': best_model, 'loss': best_val_loss}, 'best_model_mnist.pth')\n",
    "                print('Model saved')\n",
    "\n",
    "            # Early stopping - if val_loss is not improving (plus a delta e-4 as buffer) then start counter\n",
    "            # after patience of a certain number of validations, then stop training\n",
    "            elif val_losses[-1] > (best_val_loss + early_stop_delta):\n",
    "                counter += 1\n",
    "                if counter >= early_stop_patience:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "\n",
    "            #print(f\"Epoch: {epoch+1}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Train Accuracy: {train_accuracies[-1]:.4f}, Val Accuracy: {val_accuracies[-1]:.4f}\")\n",
    "            # Print the actual number of epochs trained\n",
    "            \n",
    "            print(f\"Step: {step}, Epoch: {epoch}\\tTrain Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Train Accuracy: {train_accuracies[-1]:.4f}, Val Accuracy: {val_accuracies[-1]:.4f}\")\n",
    "            #### Lave plots med meshgrid f-funktion af normalising flow undervejs for at se ændringen\n",
    "            \n",
    "            model.train()\n",
    "        \n",
    "    print(\"Finished training.\")\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies, all_train_losses #,model\n",
    "\n",
    "train_losses, val_losses, train_accuracies, val_accuracies, all_train_losses = train(postnet_model, optimiser, train_loader, test_loader, \n",
    "                                                                   num_epochs, validation_every_steps, early_stop_delta, early_stop_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss of training and validation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].plot(train_losses, label='Train Loss')\n",
    "axes[0].plot(val_losses, label='Validation Loss')\n",
    "axes[0].set_xlabel('Validation Epochs')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Train and Validation Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot accuracies of training and validation\n",
    "axes[1].plot(train_accuracies, label='Train Accuracy')\n",
    "axes[1].plot(val_accuracies, label='Validation Accuracy')\n",
    "axes[1].set_xlabel('Validation Epochs')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Train and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all train losses\n",
    "print(len(all_train_losses))\n",
    "# plot all_train_losses\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(all_train_losses,  '.',label='Train Loss', alpha=0.3)\n",
    "#plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model from pth file type\n",
    "device = torch.device(\"cpu\")\n",
    "flow_models = []\n",
    "for class_label in range(num_classes):\n",
    "    conditioner = Conditioner(in_dim=in_dim, out_dim=out_dim, num_hidden=num_hidden, hidden_dim=hidden_dim, num_params=num_params)\n",
    "    affine_coupling = AffineCouplingLayer(conditioner, split=lambda x: x.chunk(2, dim=-1)) # split the tensor into 2 parts\n",
    "    flows = [affine_coupling for _ in range(num_flows)]\n",
    "    latent_distribution = torch.distributions.MultivariateNormal(loc=torch.zeros(data_dim).to(device), scale_tril=torch.eye(data_dim).to(device))\n",
    "\n",
    "    flow_model = NormalisingFlow(latent_distribution, flows). to(device)\n",
    "    #flow_model = NormalisingFlow(latent_distribution, flows).apply(init_weights).to(device)\n",
    "    flow_models.append(flow_model)\n",
    "best_model = PosteriorNetwork(latent_dim, flow_models, N, num_classes, y_train).to(device)\n",
    "best_model.load_state_dict(torch.load('best_model_mnist.pth', map_location=device)['model_state_dict'])\n",
    "#best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test best model on test data\n",
    "N = N.to(device) #move num classes to device just in case\n",
    "alpha_values = []\n",
    "# print(next(best_model.parameters()).is_cuda)\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_index, (X_test_batch, y_test_batch) in enumerate(test_loader):\n",
    "        X_test_batch, y_test_batch = X_test_batch.to(device), y_test_batch.to(device)\n",
    "        # Forward pass to compute alpha values for this batch\n",
    "        alpha_preds = best_model(X_test_batch, N) # batch x num_classes    \n",
    "        # Append the alpha values to the list\n",
    "        #print(alpha_preds.shape)\n",
    "        alpha_values.append(alpha_preds)\n",
    "\n",
    "# Combine alpha values from all batches list into a single tensor\n",
    "alpha_values = torch.cat(alpha_values, dim=0)\n",
    "print(\"Alpha values shape: \", alpha_values.shape)\n",
    "\n",
    "# Normalize alpha values\n",
    "alphas_norm = F.normalize(alpha_values, p=1, dim=1) #reduce to dim 1, sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train_flow_model(postnet_model, optimiser, num_epochs, X_train, y_train):\n",
    "#def train_flow_model(flow_model, optimiser, X_train, num_epochs, class_label):\n",
    "    losses = []\n",
    "    #losses = torch.zeros(num_epochs)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimiser, step_size=1000, gamma=0.999)\n",
    "    for epoch in range(num_epochs):\n",
    "        optimiser.zero_grad()\n",
    "        log_prob = flow_model.log_prob(X_train)\n",
    "        loss = -torch.mean(log_prob)\n",
    "        loss.backward()\n",
    "        utils.clip_grad_norm_(flow_model.parameters(), max_norm=5.0)  # Adjust max_norm as needed\n",
    "        optimiser.step()\n",
    "        scheduler.step()\n",
    "        #losses[epoch] = loss.item()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Flow model {class_label} Epoch [{epoch + 1}/{num_epochs}], Loss: {loss:.4f}')\n",
    "    return losses, flow_model \n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    postnet_model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0  # Initialize the running loss\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data  # Get the input data and labels\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimiser.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = postnet_model(inputs, N, labels)  # Adjust this line according to your model's forward function\n",
    "        # Compute the loss\n",
    "        loss = loss(outputs, labels)  # Adjust this line according to your loss function\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update the model's parameters\n",
    "        optimiser.step()\n",
    "        \n",
    "        # Update the running loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Print statistics every 'validation_every_steps' steps\n",
    "        if i % validation_every_steps == validation_every_steps - 1:\n",
    "            print(f\"[Epoch {epoch + 1}, Step {i + 1}] Loss: {running_loss / validation_every_steps:.3f}\")\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Training\n",
    "for epoch in range(num_epochs):\n",
    "    postnet_model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        log_prob = postnet_model(images)\n",
    "        # Compute the Posterior Network loss\n",
    "        loss = posterior_network_loss(log_prob, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for this epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save the trained model if needed\n",
    "torch.save(postnet_model.state_dict(), \"posterior_net.pth\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses = []\n",
    "flow_models_trained = []\n",
    "for class_label in range(num_classes):\n",
    "    print(\"class label\",class_label)\n",
    "    flow_models[class_label].apply(init_weights)\n",
    "    losses, flow_model = train_flow_model(flow_models[class_label], optimisers[class_label], X_train[class_label], num_epochs, class_label)\n",
    "    all_losses.append(losses)\n",
    "    flow_models_trained.append(flow_model)\n",
    "all_losses = np.array(all_losses)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axes[0].plot(all_losses[0,10:])\n",
    "axes[0].set_title(\"Losses class 0\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[1].plot(all_losses[1,10:])\n",
    "axes[1].set_title(\"Losses class 1\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Loss\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "X_train = X_train.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore feature space\n",
    "def plot_latent_space(flow_model, num_samples, class_label):\n",
    "    z = flow_model.latent_sample(num_samples)\n",
    "    x = flow_model.g(z)\n",
    "    x = x.detach().cpu().numpy()\n",
    "    z = z.detach().cpu().numpy()\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(z[:, 0], z[:, 1], c=class_label, cmap='tab10', alpha=0.5)\n",
    "    plt.title('Latent space')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(x[:, 0], x[:, 1], c=class_label, cmap='tab10', alpha=0.5)\n",
    "    plt.title('Feature space')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final loss class 0: \", all_losses[0,-1])\n",
    "print(\"Final loss class 1: \", all_losses[1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples from the trained models\n",
    "num_samples_gen_per_class = 500\n",
    "generated_samples = []\n",
    "with torch.no_grad():\n",
    "    for class_label in range(num_classes):\n",
    "        samples = flow_models[class_label].sample(num_samples=num_samples_gen_per_class).cpu().numpy()\n",
    "        generated_samples.append(samples)\n",
    "generated_samples = np.array(generated_samples)\n",
    "print(\"Generated samples shape: \", generated_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if class 0 from generated samples is different from class 1 in generated samples\n",
    "# colors = ['r', 'b']\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# for class_label in range(num_classes):\n",
    "#     plt.scatter(generated_samples[class_label][:, 0], generated_samples[class_label][:, 1], \n",
    "#                 color=colors[class_label], alpha = 0.7, label=f'Generated Class {class_label}')\n",
    "# plt.xlabel('Feature 1')\n",
    "# plt.ylabel('Feature 2')\n",
    "# plt.legend()\n",
    "# plt.title('Generated Samples for Class 0 and Class 1')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "# Plot Class 0\n",
    "axes[0].scatter(X_train[0, :, 0], X_train[0, :, 1], color='r', alpha=0.6, label='Class 0')\n",
    "axes[0].scatter(generated_samples[0, :, 0], generated_samples[0, :, 1], color='g', alpha=0.6, label='Generated Class 0')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "axes[0].set_title('Class 0 vs. Generated Class 0')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot Class 1\n",
    "axes[1].scatter(X_train[1, :, 0], X_train[1, :, 1], color='b', alpha=0.6, label='Class 1')\n",
    "axes[1].scatter(generated_samples[1, :, 0], generated_samples[1, :, 1], color='g', alpha=0.6, label='Generated Class 1')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "axes[1].set_title('Class 1 vs. Generated Class 1')\n",
    "axes[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train[0, :, 0],X_train[0, :, 1], color='r', alpha=0.6, label='Class 0')\n",
    "plt.scatter(generated_samples[0, :, 0], generated_samples[0, :, 1], color='g', alpha=0.5, label='Generated Class 0')\n",
    "plt.scatter(X_train[1, :, 0], X_train[1, :, 1], color='b', alpha=0.6, label='Class 1')\n",
    "plt.scatter(generated_samples[1, :, 0], generated_samples[1, :, 1], color='y', alpha=0.5, label='Generated Class 1')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Class 0 vs. Generated Class 0')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx, ny = (100, 100)\n",
    "x = np.linspace(-4, 4, nx)\n",
    "y = np.linspace(-4, 4, ny)\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "mesh = np.stack((xv.flatten(), yv.flatten()),axis=1)\n",
    "mesh_tensor = torch.FloatTensor(mesh).to(device)\n",
    "print(mesh_tensor.shape)\n",
    "\n",
    "# class 0\n",
    "logprobs = flow_models_trained[0].log_prob(mesh_tensor).clamp(min=-10, max=10)\n",
    "logprobs = logprobs.cpu().reshape((100,100)).detach().numpy()\n",
    "# class 1\n",
    "logprobs2 = flow_models_trained[1].log_prob(mesh_tensor).clamp(min=-10, max=10)\n",
    "logprobs2 = logprobs2.cpu().reshape((100,100)).detach().numpy()\n",
    "\n",
    "\n",
    "# Plot the log probability as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(np.exp(logprobs), extent=[-4,4,-4,4], origin='lower', cmap='viridis')\n",
    "plt.scatter(X_train[0, :, 0], X_train[0, :, 1], label='Original Data', alpha=0.07, color='red')\n",
    "plt.colorbar(label='Probability Density')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Probability Density Function')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(np.exp(logprobs2), extent=[-4,4,-4,4], origin='lower', cmap='viridis')\n",
    "plt.scatter(X_train[1, :, 0], X_train[1, :, 1], label='Original Data', alpha=0.07, color='red')\n",
    "plt.colorbar(label='Probability Density')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Probability Density Function')\n",
    "plt.show()\n",
    "\n",
    "#logprobs = flow_models[class_label].log_prob(X_tensor2)\n",
    "\n",
    "# logprobs = logprobs.reshape((100,100))\n",
    "#logprobs = logprobs.cpu().reshape((100,100)).detach().numpy()\n",
    "# plt.figure(figsize = (8,8))\n",
    "# plt.imshow(np.exp(logprobs), origin='lower', extent=[5,15,5,15])\n",
    "\n",
    "# class_label = 1\n",
    "# with torch.no_grad():\n",
    "#     samples_c1 = flow_models[class_label].sample(num_samples=num_samples_gen_per_class).cpu().numpy()\n",
    "\n",
    "# #plt.scatter(synthetic_data[:, 0],synthetic_data[:, 1], label='Generated Samples', alpha=0.05, color='yellow')\n",
    "# plt.scatter(X_train[:, 0], X_train[:, 1], label='Original Data', alpha=0.07, color='red')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = StandardScaler()\n",
    "#X_train = torch.stack([torch.FloatTensor(scaler.fit_transform(X_class)) for X_class in X_train])\n",
    "\n",
    "# Data load\n",
    "num_classes = 2\n",
    "num_samples_per_class = 500\n",
    "noise = 0.05\n",
    "X_train, y_train = make_moons(n_samples=num_classes*num_samples_per_class, noise=noise)\n",
    "X_train = torch.FloatTensor(np.array(X_train))\n",
    "y_train = np.array(y_train)\n",
    "# Multi class standardisation \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "#Plot the standardised dataset\n",
    "# Plot the dataset\n",
    "plt.figure(figsize=(5, 5))\n",
    "colors = ['r', 'b']\n",
    "for class_label in range(num_classes):\n",
    "    plt.scatter(X_train[y_train == class_label][:, 0], \n",
    "                X_train[y_train == class_label][:, 1],\n",
    "                color=colors[class_label], marker='o', label=f'Class {class_label}')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.title('Two Moons Dataset (Class 0 and Class 1)')\n",
    "plt.show()\n",
    "\n",
    "# class0 = X_train[y_train == 0]\n",
    "# class1 = X_train[y_train == 1]\n",
    "# X_train = torch.stack((class0, class1))\n",
    "# print(\"X_train shape: \", X_train.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
